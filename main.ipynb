{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eb966b6-5dc5-40cd-b7c5-219ad2f28380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "import model\n",
    "import train_loop\n",
    "import data_loader\n",
    "import visual\n",
    "import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "017f5db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "TARGET_DATA_DIR = os.path.join(DATA_DIR, \"target\")\n",
    "LIVECELL_IMG_DIR = os.path.join(DATA_DIR, \"livecell\", \"images\")\n",
    "LIVECELL_MASK_DIR = os.path.join(DATA_DIR, \"livecell\", \"masks\")\n",
    "UNITY_IMG_DIR = os.path.join(DATA_DIR, \"unity_data\", \"images\")\n",
    "UNITY_MASK_DIR = os.path.join(DATA_DIR, \"unity_data\", \"masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e551319",
   "metadata": {},
   "source": [
    "### liveCell dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b5690",
   "metadata": {},
   "source": [
    "Next cell creates dataset using only LiveCell dataset and divides dataset to train and test datasets. \n",
    "\n",
    "**Do not run this cell if you want to use combination of LivCell and synthetic dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dfba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_loader.MaskedDataset(LIVECELL_IMG_DIR, LIVECELL_MASK_DIR, length=None, in_memory=False)\n",
    "seed = 123\n",
    "test_percent = 0.001\n",
    "n_test = int(len(dataset) * test_percent)\n",
    "n_train = len(dataset) - n_test\n",
    "train_set, test_set = random_split(dataset, [n_train, n_test], generator=torch.Generator().manual_seed(123))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84812598",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Combined dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850514d3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next cell creates dataset which combines LiveCell dataset with synthetic dataset and divides dataset to train and test datasets. \n",
    "\n",
    "**Do not run this cell if you ran previous code cell to use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953374b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LC_dataset = data_loader.MaskedDataset(LIVECELL_IMG_DIR, LIVECELL_MASK_DIR, length=None, in_memory=False)\n",
    "Unity_dataset = data_loader.MaskedDataset(UNITY_IMG_DIR, UNITY_MASK_DIR, length=None, in_memory=False)\n",
    "datasets = [LC_dataset, Unity_dataset]\n",
    "dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "\n",
    "seed = 123\n",
    "test_percent = 0.001\n",
    "n_test = int(len(dataset) * test_percent)\n",
    "n_train = len(dataset) - n_test\n",
    "train_set, test_set = random_split(dataset, [n_train, n_test], generator=torch.Generator().manual_seed(123))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e2f41",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# UNet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3f69e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Unet training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cfc0d1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this part, you can create neural network and train it. This might take several hours, especially if you do not have GPU. You can change training parameters in `train.net()`.\n",
    "\n",
    "If you have already trained model, there is no need to run this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570211aa-61b6-4677-8a6d-5453a027e2a0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "neural_net = model.Unet(numChannels=1, classes=2, dropout = 0.1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "neural_net.to(device=device)\n",
    "neural_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061dee1f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_loop.train_net(   net=neural_net,\n",
    "                        dataset = train_set,\n",
    "                        epochs= 1,              # Set epochs\n",
    "                        batch_size= 2,          # Batch size\n",
    "                        learning_rate=0.001,    # Learning rate\n",
    "                        device=device,\n",
    "                        val_percent=0.1,        # Percent of test set\n",
    "                        save_checkpoint = True,\n",
    "                        amp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e0f8a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84020c2f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can load already trained model from your computer. Write the path to model file in variable `PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57c642",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cadce3a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = torch.load(PATH, map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c31e0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9176e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visual.imshow_side_by_side_model(1, test_set, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0f66a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "      image, mask = test_set[1]\n",
    "      prediction = net(torch.unsqueeze(image, dim=0))\n",
    "      prediction = torch.squeeze(prediction, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4fb8cf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(torch.squeeze(image), cmap = 'gray')\n",
    "mask_np = np.delete(prediction.permute(1,2,0).numpy(),0, axis=2)\n",
    "\n",
    "#mask_np = np.where(mask_np == 0, np.array([0,0,0]), mask_np)\n",
    "#mask_np = np.where(mask_np == 1, np.array([0,1,0]), mask_np)\n",
    "plt.imshow(mask_np, alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60cd39",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.squeeze(image), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a5a61",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.squeeze(mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a4561",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Target domain visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7835b0b9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target_set = data_loader.UnMaskedDataset(TARGET_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c706e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "      image = target_set[0]\n",
    "      print(type(image))\n",
    "      prediction = net(torch.unsqueeze(image, dim=0))\n",
    "      prediction = torch.squeeze(prediction, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11924e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.squeeze(image), cmap = 'gray')\n",
    "mask_np = np.delete(prediction.permute(1,2,0).numpy(),0, axis=2)\n",
    "\n",
    "mask_np = np.where(mask_np == 0, np.array([0,0,0]), mask_np)\n",
    "mask_np = np.where(mask_np == 1, np.array([0,1,0]), mask_np)\n",
    "plt.imshow(mask_np, alpha = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150adfb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.squeeze(image), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbade8e",
   "metadata": {},
   "source": [
    "# Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628e0ced",
   "metadata": {},
   "source": [
    "## CycleGan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c98ca5",
   "metadata": {},
   "source": [
    "### Load trained model \n",
    "\n",
    "If you have allready trained model, you can use this section to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CycleGan import Generator, ResBlock\n",
    "#from CycleGan_Vol2 import unet_generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_A2B_Path = os.path.join(os.getcwd(), \"model\", \"CycleGan_2022-05-05\",  \"final_50_epochs_with_half_empty_G_A2B.pt\" )\n",
    "G_B2A_Path = os.path.join(os.getcwd(), \"model\", \"CycleGan_2022-05-05\", \"final_50_epochs_with_half_empty_G_B2A.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' #torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "G_A2B = torch.load(G_A2B_Path, map_location=device)\n",
    "G_B2A = torch.load(G_B2A_Path, map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b9415",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e06bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Target_dataset = data_loader.UnMaskedDataset(TARGET_DATA_DIR, mode=1, IMG_SIZE=256)\n",
    "\n",
    "seed = 123\n",
    "target_test_percent = 0.01\n",
    "n_test_target = int(len(Target_dataset) * target_test_percent)\n",
    "n_train_target = len(Target_dataset) - n_test_target\n",
    "target_train_set, target_test_set = torch.utils.data.random_split(Target_dataset, [n_train_target, n_test_target], generator=torch.Generator().manual_seed(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def82041",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "      image = target_train_set[0]\n",
    "      prediction = G_B2A(image)\n",
    "      prediction2 = G_A2B(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.permute(1,2,0)\n",
    "prediction2 = prediction2.permute(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= 2 * np.array(plt.rcParams['figure.figsize']))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(torch.squeeze(image), cmap = 'gray')\n",
    "plt.title(\"Original target image\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(prediction, cmap = 'gray')\n",
    "plt.title(\"Shifted domain\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(prediction2, cmap = 'gray')\n",
    "plt.title(\"domain shifted back to original\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b2416",
   "metadata": {},
   "source": [
    "## Transforming test images domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread(os.path.join(TARGET_DATA_DIR, \"000_0.png\"))\n",
    "Target_dataset = data_loader.UnMaskedDataset(TARGET_DATA_DIR, mode=3)\n",
    "\n",
    "seed = 123\n",
    "target_test_percent = 0.01\n",
    "n_test_target = int(len(Target_dataset) * target_test_percent)\n",
    "n_train_target = len(Target_dataset) - n_test_target\n",
    "target_train_set, target_test_set = torch.utils.data.random_split(Target_dataset, [n_train_target, n_test_target], generator=torch.Generator().manual_seed(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34963eec",
   "metadata": {},
   "source": [
    "model goes whole image through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "image = target_test_set[0]\n",
    "conver_PIL = transforms.ToPILImage()\n",
    "convert_Tensor = transforms.ToTensor()\n",
    "im = conver_PIL(image)\n",
    "imgwidth, imgheight = im.size\n",
    "images = []\n",
    "row = 0\n",
    "for i in range(0,imgheight,block_size):\n",
    "    images.append([])\n",
    "    for j in range(0,imgwidth,block_size):\n",
    "        box = (j, i, j+block_size, i+block_size)\n",
    "        a = im.crop(box)\n",
    "        images[row].append(a)\n",
    "    row +=1\n",
    "\n",
    "\n",
    "new_image = Image.new('L', size=(len(images[0])*block_size, len(images)*block_size))\n",
    "with torch.no_grad():\n",
    "    for row in range(len(images)):\n",
    "        i = 0\n",
    "        for img in images[row]:\n",
    "        \n",
    "            prediction = G_B2A(convert_Tensor(img))\n",
    "            new_image.paste(conver_PIL(prediction), (i*block_size, row*block_size))\n",
    "            i += 1\n",
    "\n",
    "new_image = new_image.crop((0,0, 2064, 1544))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= 3 * np.array(plt.rcParams['figure.figsize']))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im, cmap=\"gray\")\n",
    "plt.title(\"Original image\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(new_image, cmap=\"gray\")\n",
    "plt.title(\"Shifted domain\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28810287",
   "metadata": {},
   "source": [
    "## Resize function test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = transformer.to_same_size(target_test_set[0], img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= 2 * np.array(plt.rcParams['figure.figsize']))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(torch.squeeze(image), cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "LC_dataset = data_loader.MaskedDataset(LIVECELL_IMG_DIR, LIVECELL_MASK_DIR, length=None, in_memory=False, mode=3)\n",
    "#LC_empty_dataset = data_loader.EmptyLiveCELLDataset(len(LC_dataset), mode = 3)\n",
    "#LC_datasets = [LC_dataset, LC_empty_dataset]  # 50% empty, 50% actual LiveCELL images\n",
    "#dataset = torch.utils.data.ConcatDataset(LC_datasets)\n",
    "dataset = LC_dataset\n",
    "loader = DataLoader(dataset)\n",
    "transform = T.ToPILImage()\n",
    "i = 0\n",
    "for img, mask in tqdm(loader):\n",
    "    img, mask = transformer.add_fake_magnetballs(img.squeeze(0), mask.squeeze(0), min_amount = 30, max_amount = 70, max_lightness=0.15)\n",
    "    img = transform(img.squeeze(0))\n",
    "    mask = mask.numpy()\n",
    "\n",
    "    img_path = \"/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/data/livecell_magnet_balls/images/\" + str(i) + \".tif\"\n",
    "    mask_path = \"/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/data/livecell_magnet_balls/masks/\" +str(i) + \"_mask\"\n",
    "    img.save(img_path)\n",
    "    np.save(mask_path, mask)\n",
    "    i +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LC_dataset = data_loader.MaskedDataset(LIVECELL_IMG_DIR, LIVECELL_MASK_DIR, length=None, in_memory=False, mode=3)\n",
    "LC_empty_dataset = data_loader.EmptyLiveCELLDataset(3*len(LC_dataset), mode = 3)\n",
    "#LC_datasets = [LC_dataset, LC_empty_dataset]  # 50% empty, 50% actual LiveCELL images\n",
    "#dataset = torch.utils.data.ConcatDataset(LC_datasets)\n",
    "dataset = LC_empty_dataset\n",
    "loader = DataLoader(dataset)\n",
    "transform = T.ToPILImage()\n",
    "i = 0\n",
    "for img, mask in tqdm(loader):\n",
    "    img, mask = transformer.add_fake_magnetballs(img.squeeze(0), mask.squeeze(0), min_amount = 30, max_amount = 70, max_lightness=0.15)\n",
    "    img = transform(img.squeeze(0))\n",
    "    mask = mask.numpy()\n",
    "\n",
    "    img_path = \"/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/data/empty_lc_magnet_balls/images/\" + \"empty_\" + str(i) + \".tif\"\n",
    "    mask_path = \"/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/data/empty_lc_magnet_balls/masks/\" +\"empty_\" + str(i) + \"_mask\"\n",
    "    img.save(img_path)\n",
    "    np.save(mask_path, mask)\n",
    "    i +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e4d90",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d145ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions\n",
    "from torchvision import transforms\n",
    "from CycleGan import ResBlock, Generator, Discriminator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94716f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_DIR = os.path.join(os.getcwd(), \"model\")\n",
    "Unet_LC_mb_mix_path = os.path.join(MODEL_DIR, \"Unet_LC_mb_mix.pth\")\n",
    "Unet_LC_mb_path = os.path.join(MODEL_DIR, \"Unet_LC_mb.pth\")\n",
    "Unet_LC_path = os.path.join(MODEL_DIR, \"Unet_LC.pth\")\n",
    "\n",
    "CG_LC_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05\", \"cyclegan_LC_G_B2A.pt\")\n",
    "CG_UN_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_2\", \"cyclegan_unity_G_B2A.pt\")\n",
    "CG_mix_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_3\", \"cyclegan_mix_G_B2A.pt\")\n",
    "CG_LC_e_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_4\", \"cyclegan_lce_G_B2A.pt\")\n",
    "CG_LC_mb_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_5\", \"cyclegan_lc_mb_G_B2A.pt\")\n",
    "CG_LC_mb_e_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_6\", \"cyclegan_lc_mb_e_G_B2A.pt\")\n",
    "CG_LC_mb_mix_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_7\", \"cyclegan_lc_mb_mix_G_B2A.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd8a76e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Unet:\n\tsize mismatch for outc.weight: copying a param with shape torch.Size([1, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 64, 1, 1]).\n\tsize mismatch for outc.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/main.ipynb Cell 54'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkosh.aalto.fi/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/main.ipynb#ch0000054vscode-remote?line=0'>1</a>\u001b[0m Unet_LC_mb \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mUnet()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkosh.aalto.fi/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/main.ipynb#ch0000054vscode-remote?line=1'>2</a>\u001b[0m Unet_LC_mb_mix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(Unet_LC_mb_mix_path, map_location\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkosh.aalto.fi/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/main.ipynb#ch0000054vscode-remote?line=2'>3</a>\u001b[0m Unet_LC_mb\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(Unet_LC_mb_path, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(device)))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkosh.aalto.fi/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/main.ipynb#ch0000054vscode-remote?line=3'>4</a>\u001b[0m Unet_LC \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(Unet_LC_path, map_location\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   <a href='file:///u/09/huttuna6/unix/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1491'>1492</a>\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   <a href='file:///u/09/huttuna6/unix/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1492'>1493</a>\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   <a href='file:///u/09/huttuna6/unix/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1493'>1494</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   <a href='file:///u/09/huttuna6/unix/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1495'>1496</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> <a href='file:///u/09/huttuna6/unix/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1496'>1497</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   <a href='file:///u/09/huttuna6/unix/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1497'>1498</a>\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   <a href='file:///u/09/huttuna6/unix/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1498'>1499</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Unet:\n\tsize mismatch for outc.weight: copying a param with shape torch.Size([1, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 64, 1, 1]).\n\tsize mismatch for outc.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2])."
     ]
    }
   ],
   "source": [
    "Unet_LC_mb = model.Unet()\n",
    "Unet_LC_mb_mix = torch.load(Unet_LC_mb_mix_path, map_location=device)\n",
    "Unet_LC_mb.load_state_dict(torch.load(Unet_LC_mb_path, map_location=torch.device(device)))\n",
    "Unet_LC = torch.load(Unet_LC_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dbfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CG_LC_G_B2A = torch.load(CG_LC_path, map_location=device)\n",
    "CG_UN_G_B2A = torch.load(CG_UN_path, map_location=device)\n",
    "CG_mix_G_B2A = torch.load(CG_mix_path, map_location=device)\n",
    "CG_LC_e_G_B2A = torch.load(CG_LC_e_path, map_location=device)\n",
    "CG_LC_mb_G_B2A = torch.load(CG_LC_mb_path, map_location=device)\n",
    "CG_LC_mb_e_G_B2A = torch.load(CG_LC_mb_e_path, map_location=device)\n",
    "CG_LC_mb_mix_G_B2A = torch.load(CG_LC_mb_mix_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c562c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = data_loader.MaskedDataset(os.path.join(DATA_DIR , \"test_target_data\", \"target_test\"), os.path.join(DATA_DIR , \"test_target_data\", \"target_annotations\"), mode=3)\n",
    "test_loader = DataLoader(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa0ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "def domain_shift(Generator, image, block_size):\n",
    "    conver_PIL = transforms.ToPILImage()\n",
    "    convert_Tensor = transforms.ToTensor()\n",
    "    im = conver_PIL(image)\n",
    "    imgwidth, imgheight = im.size\n",
    "    images = []\n",
    "    row = 0\n",
    "    for i in range(0,imgheight,block_size):\n",
    "        images.append([])\n",
    "        for j in range(0,imgwidth,block_size):\n",
    "            box = (j, i, j+block_size, i+block_size)\n",
    "            a = im.crop(box)\n",
    "            images[row].append(a)\n",
    "        row +=1\n",
    "\n",
    "\n",
    "    new_image = Image.new('L', size=(len(images[0])*block_size, len(images)*block_size))\n",
    "    with torch.no_grad():\n",
    "        for row in range(len(images)):\n",
    "            i = 0\n",
    "            for img in images[row]:\n",
    "            \n",
    "                prediction = Generator(convert_Tensor(img))\n",
    "                new_image.paste(conver_PIL(prediction), (i*block_size, row*block_size))\n",
    "                i += 1\n",
    "\n",
    "    new_image = new_image.crop((0,0, 2064, 1544))\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "020cc0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/main.ipynb Cell 58'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkosh.aalto.fi/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/main.ipynb#ch0000058vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m img, mask \u001b[39min\u001b[39;00m tqdm(test_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkosh.aalto.fi/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/main.ipynb#ch0000058vscode-remote?line=3'>4</a>\u001b[0m     new_img \u001b[39m=\u001b[39m domain_shift(CG_LC_mb_G_B2A, img\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m), \u001b[39m512\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkosh.aalto.fi/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/main.ipynb#ch0000058vscode-remote?line=4'>5</a>\u001b[0m     new_mask \u001b[39m=\u001b[39m Unet_LC_mb(new_img)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkosh.aalto.fi/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/main.ipynb#ch0000058vscode-remote?line=5'>6</a>\u001b[0m     plt\u001b[39m.\u001b[39msubplot(\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkosh.aalto.fi/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/main.ipynb#ch0000058vscode-remote?line=6'>7</a>\u001b[0m     plt\u001b[39m.\u001b[39mimshow(new_img, cmap \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize= 2 * np.array(plt.rcParams['figure.figsize']))\n",
    "it = 1 \n",
    "for img, mask in tqdm(test_loader):\n",
    "    new_img = domain_shift(CG_LC_mb_G_B2A, img.squeeze(0), 512)\n",
    "    new_mask = Unet_LC_mb(new_img)\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.imshow(new_img, cmap = 'gray')\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.imshow(new_mask, cmap = 'gray')\n",
    "    it += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82844003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
