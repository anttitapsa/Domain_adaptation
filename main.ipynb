{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb966b6-5dc5-40cd-b7c5-219ad2f28380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/09/huttuna6/unix/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F \n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "import model\n",
    "import train_loop\n",
    "import data_loader\n",
    "import visual\n",
    "import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017f5db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "TARGET_DATA_DIR = os.path.join(DATA_DIR, \"target\")\n",
    "LIVECELL_IMG_DIR = os.path.join(DATA_DIR, \"livecell\", \"images\")\n",
    "LIVECELL_MASK_DIR = os.path.join(DATA_DIR, \"livecell\", \"masks\")\n",
    "UNITY_IMG_DIR = os.path.join(DATA_DIR, \"unity_data\", \"images\")\n",
    "UNITY_MASK_DIR = os.path.join(DATA_DIR, \"unity_data\", \"masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e551319",
   "metadata": {},
   "source": [
    "### liveCell dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b5690",
   "metadata": {},
   "source": [
    "Next cell creates dataset using only LiveCell dataset and divides dataset to train and test datasets. \n",
    "\n",
    "**Do not run this cell if you want to use combination of LivCell and synthetic dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dfba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_loader.MaskedDataset(LIVECELL_IMG_DIR, LIVECELL_MASK_DIR, length=None, in_memory=False)\n",
    "seed = 123\n",
    "test_percent = 0.001\n",
    "n_test = int(len(dataset) * test_percent)\n",
    "n_train = len(dataset) - n_test\n",
    "train_set, test_set = random_split(dataset, [n_train, n_test], generator=torch.Generator().manual_seed(123))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84812598",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Combined dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850514d3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next cell creates dataset which combines LiveCell dataset with synthetic dataset and divides dataset to train and test datasets. \n",
    "\n",
    "**Do not run this cell if you ran previous code cell to use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953374b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LC_dataset = data_loader.MaskedDataset(LIVECELL_IMG_DIR, LIVECELL_MASK_DIR, length=None, in_memory=False)\n",
    "Unity_dataset = data_loader.MaskedDataset(UNITY_IMG_DIR, UNITY_MASK_DIR, length=None, in_memory=False)\n",
    "datasets = [LC_dataset, Unity_dataset]\n",
    "dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "\n",
    "seed = 123\n",
    "test_percent = 0.001\n",
    "n_test = int(len(dataset) * test_percent)\n",
    "n_train = len(dataset) - n_test\n",
    "train_set, test_set = random_split(dataset, [n_train, n_test], generator=torch.Generator().manual_seed(123))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e2f41",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# UNet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3f69e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Unet training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cfc0d1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this part, you can create neural network and train it. This might take several hours, especially if you do not have GPU. You can change training parameters in `train.net()`.\n",
    "\n",
    "If you have already trained model, there is no need to run this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570211aa-61b6-4677-8a6d-5453a027e2a0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "neural_net = model.Unet(numChannels=1, classes=2, dropout = 0.1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "neural_net.to(device=device)\n",
    "neural_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061dee1f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_loop.train_net(   net=neural_net,\n",
    "                        dataset = train_set,\n",
    "                        epochs= 1,              # Set epochs\n",
    "                        batch_size= 2,          # Batch size\n",
    "                        learning_rate=0.001,    # Learning rate\n",
    "                        device=device,\n",
    "                        val_percent=0.1,        # Percent of test set\n",
    "                        save_checkpoint = True,\n",
    "                        amp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e0f8a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84020c2f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can load already trained model from your computer. Write the path to model file in variable `PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57c642",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cadce3a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = torch.load(PATH, map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c31e0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9176e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visual.imshow_side_by_side_model(1, test_set, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0f66a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "      image, mask = test_set[1]\n",
    "      prediction = net(torch.unsqueeze(image, dim=0))\n",
    "      prediction = torch.squeeze(prediction, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4fb8cf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(torch.squeeze(image), cmap = 'gray')\n",
    "mask_np = np.delete(prediction.permute(1,2,0).numpy(),0, axis=2)\n",
    "\n",
    "#mask_np = np.where(mask_np == 0, np.array([0,0,0]), mask_np)\n",
    "#mask_np = np.where(mask_np == 1, np.array([0,1,0]), mask_np)\n",
    "plt.imshow(mask_np, alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60cd39",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.squeeze(image), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a5a61",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.squeeze(mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a4561",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Target domain visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7835b0b9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target_set = data_loader.UnMaskedDataset(TARGET_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c706e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "      image = target_set[0]\n",
    "      print(type(image))\n",
    "      prediction = net(torch.unsqueeze(image, dim=0))\n",
    "      prediction = torch.squeeze(prediction, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11924e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.squeeze(image), cmap = 'gray')\n",
    "mask_np = np.delete(prediction.permute(1,2,0).numpy(),0, axis=2)\n",
    "\n",
    "mask_np = np.where(mask_np == 0, np.array([0,0,0]), mask_np)\n",
    "mask_np = np.where(mask_np == 1, np.array([0,1,0]), mask_np)\n",
    "plt.imshow(mask_np, alpha = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150adfb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.squeeze(image), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbade8e",
   "metadata": {},
   "source": [
    "# Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628e0ced",
   "metadata": {},
   "source": [
    "## CycleGan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c98ca5",
   "metadata": {},
   "source": [
    "### Load trained model \n",
    "\n",
    "If you have allready trained model, you can use this section to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CycleGan import Generator, ResBlock\n",
    "#from CycleGan_Vol2 import unet_generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_A2B_Path = os.path.join(os.getcwd(), \"model\", \"CycleGan_2022-05-05\",  \"final_50_epochs_with_half_empty_G_A2B.pt\" )\n",
    "G_B2A_Path = os.path.join(os.getcwd(), \"model\", \"CycleGan_2022-05-05\", \"final_50_epochs_with_half_empty_G_B2A.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' #torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "G_A2B = torch.load(G_A2B_Path, map_location=device)\n",
    "G_B2A = torch.load(G_B2A_Path, map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b9415",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e06bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Target_dataset = data_loader.UnMaskedDataset(TARGET_DATA_DIR, mode=1, IMG_SIZE=256)\n",
    "\n",
    "seed = 123\n",
    "target_test_percent = 0.01\n",
    "n_test_target = int(len(Target_dataset) * target_test_percent)\n",
    "n_train_target = len(Target_dataset) - n_test_target\n",
    "target_train_set, target_test_set = torch.utils.data.random_split(Target_dataset, [n_train_target, n_test_target], generator=torch.Generator().manual_seed(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def82041",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "      image = target_train_set[0]\n",
    "      prediction = G_B2A(image)\n",
    "      prediction2 = G_A2B(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.permute(1,2,0)\n",
    "prediction2 = prediction2.permute(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= 2 * np.array(plt.rcParams['figure.figsize']))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(torch.squeeze(image), cmap = 'gray')\n",
    "plt.title(\"Original target image\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(prediction, cmap = 'gray')\n",
    "plt.title(\"Shifted domain\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(prediction2, cmap = 'gray')\n",
    "plt.title(\"domain shifted back to original\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b2416",
   "metadata": {},
   "source": [
    "## Transforming test images domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread(os.path.join(TARGET_DATA_DIR, \"000_0.png\"))\n",
    "Target_dataset = data_loader.UnMaskedDataset(TARGET_DATA_DIR, mode=3)\n",
    "\n",
    "seed = 123\n",
    "target_test_percent = 0.01\n",
    "n_test_target = int(len(Target_dataset) * target_test_percent)\n",
    "n_train_target = len(Target_dataset) - n_test_target\n",
    "target_train_set, target_test_set = torch.utils.data.random_split(Target_dataset, [n_train_target, n_test_target], generator=torch.Generator().manual_seed(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34963eec",
   "metadata": {},
   "source": [
    "model goes whole image through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "image = target_test_set[0]\n",
    "conver_PIL = transforms.ToPILImage()\n",
    "convert_Tensor = transforms.ToTensor()\n",
    "im = conver_PIL(image)\n",
    "imgwidth, imgheight = im.size\n",
    "images = []\n",
    "row = 0\n",
    "for i in range(0,imgheight,block_size):\n",
    "    images.append([])\n",
    "    for j in range(0,imgwidth,block_size):\n",
    "        box = (j, i, j+block_size, i+block_size)\n",
    "        a = im.crop(box)\n",
    "        images[row].append(a)\n",
    "    row +=1\n",
    "\n",
    "\n",
    "new_image = Image.new('L', size=(len(images[0])*block_size, len(images)*block_size))\n",
    "with torch.no_grad():\n",
    "    for row in range(len(images)):\n",
    "        i = 0\n",
    "        for img in images[row]:\n",
    "        \n",
    "            prediction = G_B2A(convert_Tensor(img))\n",
    "            new_image.paste(conver_PIL(prediction), (i*block_size, row*block_size))\n",
    "            i += 1\n",
    "\n",
    "new_image = new_image.crop((0,0, 2064, 1544))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= 3 * np.array(plt.rcParams['figure.figsize']))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im, cmap=\"gray\")\n",
    "plt.title(\"Original image\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(new_image, cmap=\"gray\")\n",
    "plt.title(\"Shifted domain\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28810287",
   "metadata": {},
   "source": [
    "## Resize function test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = transformer.to_same_size(target_test_set[0], img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= 2 * np.array(plt.rcParams['figure.figsize']))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(torch.squeeze(image), cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "LC_dataset = data_loader.MaskedDataset(LIVECELL_IMG_DIR, LIVECELL_MASK_DIR, length=None, in_memory=False, mode=3)\n",
    "#LC_empty_dataset = data_loader.EmptyLiveCELLDataset(len(LC_dataset), mode = 3)\n",
    "#LC_datasets = [LC_dataset, LC_empty_dataset]  # 50% empty, 50% actual LiveCELL images\n",
    "#dataset = torch.utils.data.ConcatDataset(LC_datasets)\n",
    "dataset = LC_dataset\n",
    "loader = DataLoader(dataset)\n",
    "transform = T.ToPILImage()\n",
    "i = 0\n",
    "for img, mask in tqdm(loader):\n",
    "    img, mask = transformer.add_fake_magnetballs(img.squeeze(0), mask.squeeze(0), min_amount = 30, max_amount = 70, max_lightness=0.15)\n",
    "    img = transform(img.squeeze(0))\n",
    "    mask = mask.numpy()\n",
    "\n",
    "    img_path = \"/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/data/livecell_magnet_balls/images/\" + str(i) + \".tif\"\n",
    "    mask_path = \"/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/data/livecell_magnet_balls/masks/\" +str(i) + \"_mask\"\n",
    "    img.save(img_path)\n",
    "    np.save(mask_path, mask)\n",
    "    i +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LC_dataset = data_loader.MaskedDataset(LIVECELL_IMG_DIR, LIVECELL_MASK_DIR, length=None, in_memory=False, mode=3)\n",
    "LC_empty_dataset = data_loader.EmptyLiveCELLDataset(3*len(LC_dataset), mode = 3)\n",
    "#LC_datasets = [LC_dataset, LC_empty_dataset]  # 50% empty, 50% actual LiveCELL images\n",
    "#dataset = torch.utils.data.ConcatDataset(LC_datasets)\n",
    "dataset = LC_empty_dataset\n",
    "loader = DataLoader(dataset)\n",
    "transform = T.ToPILImage()\n",
    "i = 0\n",
    "for img, mask in tqdm(loader):\n",
    "    img, mask = transformer.add_fake_magnetballs(img.squeeze(0), mask.squeeze(0), min_amount = 30, max_amount = 70, max_lightness=0.15)\n",
    "    img = transform(img.squeeze(0))\n",
    "    mask = mask.numpy()\n",
    "\n",
    "    img_path = \"/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/data/empty_lc_magnet_balls/images/\" + \"empty_\" + str(i) + \".tif\"\n",
    "    mask_path = \"/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/data/empty_lc_magnet_balls/masks/\" +\"empty_\" + str(i) + \"_mask\"\n",
    "    img.save(img_path)\n",
    "    np.save(mask_path, mask)\n",
    "    i +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e4d90",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d145ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions\n",
    "from torchvision import transforms\n",
    "from CycleGan import ResBlock, Generator, Discriminator\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94716f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_DIR = os.path.join(os.getcwd(), \"model\")\n",
    "Unet_LC_mb_mix_path = os.path.join(MODEL_DIR, \"Unet_LC_mb_mix.pth\")\n",
    "Unet_LC_mb_path = os.path.join(MODEL_DIR, \"Unet_AugmentedLiveCell_20epochs_5batch_final.pth\")\n",
    "Unet_LC_path = os.path.join(MODEL_DIR, \"Unet_LC.pth\")\n",
    "\n",
    "CG_LC_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05\", \"cyclegan_LC_G_B2A.pt\")\n",
    "CG_UN_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_2\", \"cyclegan_unity_G_B2A.pt\")\n",
    "CG_mix_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_3\", \"cyclegan_mix_G_B2A.pt\")\n",
    "CG_LC_e_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_4\", \"cyclegan_lce_G_B2A.pt\")\n",
    "CG_LC_mb_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_5\", \"cyclegan_lc_mb_G_B2A.pt\")\n",
    "CG_LC_mb_e_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_6\", \"cyclegan_lc_mb_e_G_B2A.pt\")\n",
    "CG_LC_mb_mix_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_7\", \"cyclegan_lc_mb_mix_G_B2A.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8a76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet_LC_mb = model.Unet(numChannels=1, classes=2, dropout = 0.1)\n",
    "Unet_LC_mb_mix = torch.load(Unet_LC_mb_mix_path, map_location=device)\n",
    "Unet_LC_mb.load_state_dict(torch.load(Unet_LC_mb_path, map_location=torch.device(device)))\n",
    "Unet_LC = torch.load(Unet_LC_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52dbfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CG_LC_G_B2A = torch.load(CG_LC_path, map_location=device)\n",
    "CG_UN_G_B2A = torch.load(CG_UN_path, map_location=device)\n",
    "CG_mix_G_B2A = torch.load(CG_mix_path, map_location=device)\n",
    "CG_LC_e_G_B2A = torch.load(CG_LC_e_path, map_location=device)\n",
    "CG_LC_mb_G_B2A = torch.load(CG_LC_mb_path, map_location=device)\n",
    "CG_LC_mb_e_G_B2A = torch.load(CG_LC_mb_e_path, map_location=device)\n",
    "CG_LC_mb_mix_G_B2A = torch.load(CG_LC_mb_mix_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c562c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 1448.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_set = data_loader.MaskedDataset(os.path.join(DATA_DIR , \"test_target_data\", \"target_test\"), os.path.join(DATA_DIR , \"test_target_data\", \"target_annotations\"), mode=3)\n",
    "test_loader = DataLoader(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aa0ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "def domain_shift(Generator, image, block_size):\n",
    "    conver_PIL = transforms.ToPILImage()\n",
    "    convert_Tensor = transforms.ToTensor()\n",
    "    im = conver_PIL(image)\n",
    "    imgwidth, imgheight = im.size\n",
    "    images = []\n",
    "    row = 0\n",
    "    for i in range(0,imgheight,block_size):\n",
    "        images.append([])\n",
    "        for j in range(0,imgwidth,block_size):\n",
    "            box = (j, i, j+block_size, i+block_size)\n",
    "            a = im.crop(box)\n",
    "            images[row].append(a)\n",
    "        row +=1\n",
    "\n",
    "\n",
    "    new_image = Image.new('L', size=(len(images[0])*block_size, len(images)*block_size))\n",
    "    with torch.no_grad():\n",
    "        for row in range(len(images)):\n",
    "            i = 0\n",
    "            for img in images[row]:\n",
    "            \n",
    "                prediction = Generator(convert_Tensor(img))\n",
    "                new_image.paste(conver_PIL(prediction), (i*block_size, row*block_size))\n",
    "                i += 1\n",
    "\n",
    "    new_image = new_image.crop((0,0, 2064, 1544))\n",
    "    return convert_Tensor(new_image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffe9a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(Generator, image, block_size):\n",
    "    conver_PIL = transforms.ToPILImage()\n",
    "    convert_Tensor = transforms.ToTensor()\n",
    "    im = conver_PIL(image)\n",
    "    imgwidth, imgheight = im.size\n",
    "    images = []\n",
    "    row = 0\n",
    "    for i in range(0,imgheight,block_size):\n",
    "        images.append([])\n",
    "        for j in range(0,imgwidth,block_size):\n",
    "            box = (j, i, j+block_size, i+block_size)\n",
    "            a = im.crop(box)\n",
    "            images[row].append(a)\n",
    "        row +=1\n",
    "\n",
    "\n",
    "    new_image = Image.new('L', size=(len(images[0])*block_size, len(images)*block_size))\n",
    "    with torch.no_grad():\n",
    "        for row in range(len(images)):\n",
    "            i = 0\n",
    "            for img in images[row]:\n",
    "            \n",
    "                prediction = Generator(convert_Tensor(img).unsqueeze(0))\n",
    "                prediction = (F.softmax(prediction, dim=1)).float()\n",
    "                prediction = prediction[0,1,:,:]\n",
    "                new_image.paste(conver_PIL(prediction.squeeze(0)), (i*block_size, row*block_size))\n",
    "                i += 1\n",
    "\n",
    "    new_image = new_image.crop((0,0, 2064, 1544))\n",
    "    return convert_Tensor(new_image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "020cc0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(Unet, CG, name):\n",
    "    fig = plt.figure(figsize= 2 * np.array(plt.rcParams['figure.figsize']))\n",
    "    it = 1\n",
    "    mask_for_eval =[] \n",
    "    for img, mask in tqdm(test_loader):\n",
    "        new_img = domain_shift(CG, img.squeeze(0), 512)\n",
    "        new_mask = create_mask(Unet,new_img.squeeze(0), 512)\n",
    "        mask_for_eval.append((mask, new_mask))\n",
    "        plt.subplot(1,4,1)\n",
    "        plt.imshow(img.squeeze(0).permute((1,2,0)), cmap = 'gray')\n",
    "        plt.title(\"Original image\")\n",
    "        plt.subplot(1,4,2)\n",
    "        plt.imshow(new_img.squeeze(0).permute((1,2,0)), cmap = 'gray')\n",
    "        plt.title(\"Shifted domain\")\n",
    "        plt.subplot(1,4,3)\n",
    "        plt.imshow(mask.permute((1,2,0)), cmap = 'gray')\n",
    "        plt.title(\"True mask\")\n",
    "        plt.subplot(1,4,4)\n",
    "        plt.imshow(new_mask.squeeze(0).permute(1,2,0), cmap = 'gray')\n",
    "        plt.title(\"Predicted mask\")\n",
    "        fig.tight_layout()\n",
    "        savepath = os.path.join(\"/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/model/images\", name, str(it) +\".svg\")\n",
    "        plt.savefig(savepath, format=\"svg\")\n",
    "        it += 1\n",
    "    \n",
    "    return mask_for_eval\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c3882c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(mask_list, name):\n",
    "    scores = [name]\n",
    "    for mask, prediction in mask_list:\n",
    "        score = functions.dice_loss(mask, prediction.squeeze(0))\n",
    "        scores.append(score)\n",
    "    scores.append(sum(scores[1:])/15)\n",
    "    f = open('/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/model/images/scores.csv', 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82844003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [12:40<02:01, 60.85s/it]"
     ]
    }
   ],
   "source": [
    "LC_mb_masks = make_predictions(Unet_LC_mb, CG_LC_mb_G_B2A, \"LC_mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(LC_mb_masks, \"LC_mb\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
