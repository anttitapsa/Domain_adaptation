{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb966b6-5dc5-40cd-b7c5-219ad2f28380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F \n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "import model\n",
    "import train_loop\n",
    "import data_loader\n",
    "import visual\n",
    "import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017f5db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "TARGET_DATA_DIR = os.path.join(DATA_DIR, \"target\")\n",
    "LIVECELL_IMG_DIR = os.path.join(DATA_DIR, \"livecell\", \"images\")\n",
    "LIVECELL_MASK_DIR = os.path.join(DATA_DIR, \"livecell\", \"masks\")\n",
    "UNITY_IMG_DIR = os.path.join(DATA_DIR, \"unity_data\", \"images\")\n",
    "UNITY_MASK_DIR = os.path.join(DATA_DIR, \"unity_data\", \"masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e551319",
   "metadata": {},
   "source": [
    "### liveCell dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b5690",
   "metadata": {},
   "source": [
    "Next cell creates dataset using only LiveCell dataset and divides dataset to train and test datasets. \n",
    "\n",
    "**Do not run this cell if you want to use combination of LivCell and synthetic dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dfba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_loader.MaskedDataset(LIVECELL_IMG_DIR, LIVECELL_MASK_DIR, length=None, in_memory=False)\n",
    "seed = 123\n",
    "test_percent = 0.001\n",
    "n_test = int(len(dataset) * test_percent)\n",
    "n_train = len(dataset) - n_test\n",
    "train_set, test_set = random_split(dataset, [n_train, n_test], generator=torch.Generator().manual_seed(123))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84812598",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Combined dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850514d3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next cell creates dataset which combines LiveCell dataset with synthetic dataset and divides dataset to train and test datasets. \n",
    "\n",
    "**Do not run this cell if you ran previous code cell to use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953374b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LC_dataset = data_loader.MaskedDataset(LIVECELL_IMG_DIR, LIVECELL_MASK_DIR, length=None, in_memory=False)\n",
    "Unity_dataset = data_loader.MaskedDataset(UNITY_IMG_DIR, UNITY_MASK_DIR, length=None, in_memory=False)\n",
    "datasets = [LC_dataset, Unity_dataset]\n",
    "dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "\n",
    "seed = 123\n",
    "test_percent = 0.001\n",
    "n_test = int(len(dataset) * test_percent)\n",
    "n_train = len(dataset) - n_test\n",
    "train_set, test_set = random_split(dataset, [n_train, n_test], generator=torch.Generator().manual_seed(123))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e2f41",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# UNet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3f69e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Unet training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cfc0d1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this part, you can create neural network and train it. This might take several hours, especially if you do not have GPU. You can change training parameters in `train.net()`.\n",
    "\n",
    "If you have already trained model, there is no need to run this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570211aa-61b6-4677-8a6d-5453a027e2a0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "neural_net = model.Unet(numChannels=1, classes=2, dropout = 0.1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "neural_net.to(device=device)\n",
    "neural_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061dee1f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_loop.train_net(   net=neural_net,\n",
    "                        dataset = train_set,\n",
    "                        epochs= 1,              # Set epochs\n",
    "                        batch_size= 2,          # Batch size\n",
    "                        learning_rate=0.001,    # Learning rate\n",
    "                        device=device,\n",
    "                        val_percent=0.1,        # Percent of test set\n",
    "                        save_checkpoint = True,\n",
    "                        amp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e0f8a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84020c2f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can load already trained model from your computer. Write the path to model file in variable `PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57c642",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cadce3a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = torch.load(PATH, map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c31e0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9176e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visual.imshow_side_by_side_model(1, test_set, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0f66a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "      image, mask = test_set[1]\n",
    "      prediction = net(torch.unsqueeze(image, dim=0))\n",
    "      prediction = torch.squeeze(prediction, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4fb8cf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(torch.squeeze(image), cmap = 'gray')\n",
    "mask_np = np.delete(prediction.permute(1,2,0).numpy(),0, axis=2)\n",
    "\n",
    "#mask_np = np.where(mask_np == 0, np.array([0,0,0]), mask_np)\n",
    "#mask_np = np.where(mask_np == 1, np.array([0,1,0]), mask_np)\n",
    "plt.imshow(mask_np, alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60cd39",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.squeeze(image), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a5a61",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.squeeze(mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a4561",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Target domain visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7835b0b9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target_set = data_loader.UnMaskedDataset(TARGET_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c706e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "      image = target_set[0]\n",
    "      print(type(image))\n",
    "      prediction = net(torch.unsqueeze(image, dim=0))\n",
    "      prediction = torch.squeeze(prediction, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11924e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.squeeze(image), cmap = 'gray')\n",
    "mask_np = np.delete(prediction.permute(1,2,0).numpy(),0, axis=2)\n",
    "\n",
    "mask_np = np.where(mask_np == 0, np.array([0,0,0]), mask_np)\n",
    "mask_np = np.where(mask_np == 1, np.array([0,1,0]), mask_np)\n",
    "plt.imshow(mask_np, alpha = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150adfb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.squeeze(image), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbade8e",
   "metadata": {},
   "source": [
    "# Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628e0ced",
   "metadata": {},
   "source": [
    "## CycleGan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c98ca5",
   "metadata": {},
   "source": [
    "### Load trained model \n",
    "\n",
    "If you have allready trained model, you can use this section to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CycleGan import Generator, ResBlock\n",
    "#from CycleGan_Vol2 import unet_generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_A2B_Path = os.path.join(os.getcwd(), \"model\", \"CycleGan_2022-05-05\",  \"final_50_epochs_with_half_empty_G_A2B.pt\" )\n",
    "G_B2A_Path = os.path.join(os.getcwd(), \"model\", \"CycleGan_2022-05-05\", \"final_50_epochs_with_half_empty_G_B2A.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' #torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "G_A2B = torch.load(G_A2B_Path, map_location=device)\n",
    "G_B2A = torch.load(G_B2A_Path, map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b9415",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e06bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Target_dataset = data_loader.UnMaskedDataset(TARGET_DATA_DIR, mode=1, IMG_SIZE=256)\n",
    "\n",
    "seed = 123\n",
    "target_test_percent = 0.01\n",
    "n_test_target = int(len(Target_dataset) * target_test_percent)\n",
    "n_train_target = len(Target_dataset) - n_test_target\n",
    "target_train_set, target_test_set = torch.utils.data.random_split(Target_dataset, [n_train_target, n_test_target], generator=torch.Generator().manual_seed(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def82041",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "      image = target_train_set[0]\n",
    "      prediction = G_B2A(image)\n",
    "      prediction2 = G_A2B(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.permute(1,2,0)\n",
    "prediction2 = prediction2.permute(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= 2 * np.array(plt.rcParams['figure.figsize']))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(torch.squeeze(image), cmap = 'gray')\n",
    "plt.title(\"Original target image\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(prediction, cmap = 'gray')\n",
    "plt.title(\"Shifted domain\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(prediction2, cmap = 'gray')\n",
    "plt.title(\"domain shifted back to original\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b2416",
   "metadata": {},
   "source": [
    "## Transforming test images domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread(os.path.join(TARGET_DATA_DIR, \"000_0.png\"))\n",
    "Target_dataset = data_loader.UnMaskedDataset(TARGET_DATA_DIR, mode=3)\n",
    "\n",
    "seed = 123\n",
    "target_test_percent = 0.01\n",
    "n_test_target = int(len(Target_dataset) * target_test_percent)\n",
    "n_train_target = len(Target_dataset) - n_test_target\n",
    "target_train_set, target_test_set = torch.utils.data.random_split(Target_dataset, [n_train_target, n_test_target], generator=torch.Generator().manual_seed(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34963eec",
   "metadata": {},
   "source": [
    "model goes whole image through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "image = target_test_set[0]\n",
    "conver_PIL = transforms.ToPILImage()\n",
    "convert_Tensor = transforms.ToTensor()\n",
    "im = conver_PIL(image)\n",
    "imgwidth, imgheight = im.size\n",
    "images = []\n",
    "row = 0\n",
    "for i in range(0,imgheight,block_size):\n",
    "    images.append([])\n",
    "    for j in range(0,imgwidth,block_size):\n",
    "        box = (j, i, j+block_size, i+block_size)\n",
    "        a = im.crop(box)\n",
    "        images[row].append(a)\n",
    "    row +=1\n",
    "\n",
    "\n",
    "new_image = Image.new('L', size=(len(images[0])*block_size, len(images)*block_size))\n",
    "with torch.no_grad():\n",
    "    for row in range(len(images)):\n",
    "        i = 0\n",
    "        for img in images[row]:\n",
    "        \n",
    "            prediction = G_B2A(convert_Tensor(img))\n",
    "            new_image.paste(conver_PIL(prediction), (i*block_size, row*block_size))\n",
    "            i += 1\n",
    "\n",
    "new_image = new_image.crop((0,0, 2064, 1544))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= 3 * np.array(plt.rcParams['figure.figsize']))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im, cmap=\"gray\")\n",
    "plt.title(\"Original image\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(new_image, cmap=\"gray\")\n",
    "plt.title(\"Shifted domain\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28810287",
   "metadata": {},
   "source": [
    "## Resize function test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = transformer.to_same_size(target_test_set[0], img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= 2 * np.array(plt.rcParams['figure.figsize']))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(torch.squeeze(image), cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "LC_dataset = data_loader.MaskedDataset(LIVECELL_IMG_DIR, LIVECELL_MASK_DIR, length=None, in_memory=False, mode=3)\n",
    "#LC_empty_dataset = data_loader.EmptyLiveCELLDataset(len(LC_dataset), mode = 3)\n",
    "#LC_datasets = [LC_dataset, LC_empty_dataset]  # 50% empty, 50% actual LiveCELL images\n",
    "#dataset = torch.utils.data.ConcatDataset(LC_datasets)\n",
    "dataset = LC_dataset\n",
    "loader = DataLoader(dataset)\n",
    "transform = T.ToPILImage()\n",
    "i = 0\n",
    "for img, mask in tqdm(loader):\n",
    "    img, mask = transformer.add_fake_magnetballs(img.squeeze(0), mask.squeeze(0), min_amount = 30, max_amount = 70, max_lightness=0.15)\n",
    "    img = transform(img.squeeze(0))\n",
    "    mask = mask.numpy()\n",
    "\n",
    "    img_path = \"/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/data/livecell_magnet_balls/images/\" + str(i) + \".tif\"\n",
    "    mask_path = \"/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/data/livecell_magnet_balls/masks/\" +str(i) + \"_mask\"\n",
    "    img.save(img_path)\n",
    "    np.save(mask_path, mask)\n",
    "    i +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LC_dataset = data_loader.MaskedDataset(LIVECELL_IMG_DIR, LIVECELL_MASK_DIR, length=None, in_memory=False, mode=3)\n",
    "LC_empty_dataset = data_loader.EmptyLiveCELLDataset(3*len(LC_dataset), mode = 3)\n",
    "#LC_datasets = [LC_dataset, LC_empty_dataset]  # 50% empty, 50% actual LiveCELL images\n",
    "#dataset = torch.utils.data.ConcatDataset(LC_datasets)\n",
    "dataset = LC_empty_dataset\n",
    "loader = DataLoader(dataset)\n",
    "transform = T.ToPILImage()\n",
    "i = 0\n",
    "for img, mask in tqdm(loader):\n",
    "    img, mask = transformer.add_fake_magnetballs(img.squeeze(0), mask.squeeze(0), min_amount = 30, max_amount = 70, max_lightness=0.15)\n",
    "    img = transform(img.squeeze(0))\n",
    "    mask = mask.numpy()\n",
    "\n",
    "    img_path = \"/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/data/empty_lc_magnet_balls/images/\" + \"empty_\" + str(i) + \".tif\"\n",
    "    mask_path = \"/u/09/huttuna6/unix/Documents/LST_GIT/lst-project/data/empty_lc_magnet_balls/masks/\" +\"empty_\" + str(i) + \"_mask\"\n",
    "    img.save(img_path)\n",
    "    np.save(mask_path, mask)\n",
    "    i +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e4d90",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d145ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions\n",
    "from torchvision import transforms\n",
    "from CycleGan import ResBlock, Generator, Discriminator\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94716f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_DIR = os.path.join(os.getcwd(), \"model\")\n",
    "Unet_LC_mb_mix_path = os.path.join(MODEL_DIR, \"Unet_Combined_20epochs_5batch_final.pth\")\n",
    "Unet_LC_mb_path = os.path.join(MODEL_DIR, \"Unet_AugmentedLiveCell_20epochs_5batch_final.pth\")\n",
    "Unet_LC_path = os.path.join(MODEL_DIR, \"Unet_LiveCell_20epochs_5batch_final.pth\")\n",
    "\n",
    "CG_LC_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05\", \"cyclegan_LC_G_B2A.pt\")\n",
    "CG_UN_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_2\", \"cyclegan_unity_G_B2A.pt\")\n",
    "CG_mix_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_3\", \"cyclegan_mix_G_B2A.pt\")\n",
    "CG_LC_e_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_4\", \"cyclegan_lce_G_B2A.pt\")\n",
    "CG_LC_mb_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_5\", \"cyclegan_lc_mb_G_B2A.pt\")\n",
    "CG_LC_mb_e_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_6\", \"cyclegan_lc_mb_e_G_B2A.pt\")\n",
    "CG_LC_mb_mix_path = os.path.join(MODEL_DIR, \"CycleGan_2022-05-05_7\", \"cyclegan_lc_mb_mix_G_B2A.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8a76e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Unet_LC_mb = model.Unet(numChannels=1, classes=2, dropout = 0.1)\n",
    "Unet_LC_mb.load_state_dict(torch.load(Unet_LC_mb_path, map_location=torch.device(device)))\n",
    "Unet_LC_mb_mix = model.Unet(numChannels=1, classes=2, dropout = 0.1)\n",
    "Unet_LC_mb_mix.load_state_dict(torch.load(Unet_LC_mb_mix_path, map_location=device))\n",
    "Unet_LC = model.Unet(numChannels=1, classes=2, dropout = 0.1)\n",
    "Unet_LC.load_state_dict(torch.load(Unet_LC_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52dbfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CG_LC_G_B2A = torch.load(CG_LC_path, map_location=device)\n",
    "CG_UN_G_B2A = torch.load(CG_UN_path, map_location=device)\n",
    "CG_mix_G_B2A = torch.load(CG_mix_path, map_location=device)\n",
    "CG_LC_e_G_B2A = torch.load(CG_LC_e_path, map_location=device)\n",
    "CG_LC_mb_G_B2A = torch.load(CG_LC_mb_path, map_location=device)\n",
    "CG_LC_mb_e_G_B2A = torch.load(CG_LC_mb_e_path, map_location=device)\n",
    "CG_LC_mb_mix_G_B2A = torch.load(CG_LC_mb_mix_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c562c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 3627.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_set = data_loader.MaskedDataset(os.path.join(DATA_DIR , \"test_target_data\", \"target_test\"), os.path.join(DATA_DIR , \"test_target_data\", \"target_annotations\"), mode=3)\n",
    "test_loader = DataLoader(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa0ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "def domain_shift(Generator, image, block_size):\n",
    "    conver_PIL = transforms.ToPILImage()\n",
    "    convert_Tensor = transforms.ToTensor()\n",
    "    im = conver_PIL(image)\n",
    "    imgwidth, imgheight = im.size\n",
    "    images = []\n",
    "    row = 0\n",
    "    for i in range(0,imgheight,block_size):\n",
    "        images.append([])\n",
    "        for j in range(0,imgwidth,block_size):\n",
    "            box = (j, i, j+block_size, i+block_size)\n",
    "            a = im.crop(box)\n",
    "            images[row].append(a)\n",
    "        row +=1\n",
    "\n",
    "\n",
    "    new_image = Image.new('L', size=(len(images[0])*block_size, len(images)*block_size))\n",
    "    with torch.no_grad():\n",
    "        for row in range(len(images)):\n",
    "            i = 0\n",
    "            for img in images[row]:\n",
    "            \n",
    "                prediction = Generator(convert_Tensor(img))\n",
    "                new_image.paste(conver_PIL(prediction), (i*block_size, row*block_size))\n",
    "                i += 1\n",
    "\n",
    "    new_image = new_image.crop((0,0, 2064, 1544))\n",
    "    return convert_Tensor(new_image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(Generator, image, block_size):\n",
    "    resize_256 = transforms.Resize((256, 256))\n",
    "    resize_orig = transforms.Resize((2064, 1544))\n",
    "    image = resize_256.forward(image)\n",
    "    conver_PIL = transforms.ToPILImage()\n",
    "    convert_Tensor = transforms.ToTensor()\n",
    "    im = conver_PIL(image)\n",
    "    imgwidth, imgheight = im.size\n",
    "    images = []\n",
    "    row = 0\n",
    "    for i in range(0,imgheight,block_size):\n",
    "        images.append([])\n",
    "        for j in range(0,imgwidth,block_size):\n",
    "            box = (j, i, j+block_size, i+block_size)\n",
    "            a = im.crop(box)\n",
    "            images[row].append(a)\n",
    "        row +=1\n",
    "\n",
    "\n",
    "    new_image = Image.new('L', size=(len(images[0])*block_size, len(images)*block_size))\n",
    "    with torch.no_grad():\n",
    "        for row in range(len(images)):\n",
    "            i = 0\n",
    "            for img in images[row]:\n",
    "            \n",
    "                prediction = Generator(convert_Tensor(img).unsqueeze(0))\n",
    "                prediction = (F.softmax(prediction, dim=1)).float()\n",
    "                prediction = prediction[0,1,:,:]\n",
    "                new_image.paste(conver_PIL(prediction.squeeze(0)), (i*block_size, row*block_size))\n",
    "                i += 1\n",
    "\n",
    "    new_image = new_image.crop((0,0, 2064, 1544))\n",
    "    return convert_Tensor(new_image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "020cc0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(Unet, CG, name):\n",
    "    fig = plt.figure(figsize= 2 * np.array(plt.rcParams['figure.figsize']))\n",
    "    it = 1\n",
    "    mask_for_eval =[]\n",
    "    resize_512 = transforms.Resize((512, 512))\n",
    "    resize_256 = transforms.Resize((256, 256))\n",
    "    resize_orig = transforms.Resize((1544, 2064))\n",
    "    for img, mask in tqdm(test_loader):\n",
    "\n",
    "        new_img = resize_256.forward(img)\n",
    "        new_img = CG(new_img)\n",
    "        new_img = resize_orig(new_img).detach()\n",
    "        #new_img = domain_shift(CG, img.squeeze(0), 512)\n",
    "        #new_mask = create_mask(Unet,new_img.squeeze(0), 512)\n",
    "        new_mask = Unet(resize_256.forward(new_img))\n",
    "        new_mask = resize_orig.forward(new_mask)\n",
    "        new_mask = (F.softmax(new_mask, dim=1)).float()\n",
    "        new_mask = new_mask[0,1,:,:].unsqueeze(0).detach()\n",
    "    \n",
    "    \n",
    "\n",
    "        mask_for_eval.append((mask, new_mask))\n",
    "        plt.subplot(1,4,1)\n",
    "        plt.imshow(img.squeeze(0).permute((1,2,0)), cmap = 'gray')\n",
    "        plt.title(\"Original image\")\n",
    "        plt.subplot(1,4,2)\n",
    "        plt.imshow(new_img.squeeze(0).permute((1,2,0)), cmap = 'gray')\n",
    "        plt.title(\"Shifted domain\")\n",
    "        plt.subplot(1,4,3)\n",
    "        plt.imshow(mask.permute((1,2,0)), cmap = 'gray')\n",
    "        plt.title(\"True mask\")\n",
    "        plt.subplot(1,4,4)\n",
    "        plt.imshow(new_mask.permute((1,2,0)), cmap = 'gray')\n",
    "        plt.title(\"Predicted mask\")\n",
    "        fig.tight_layout()\n",
    "        savepath = os.path.join(os.getcwd(),\"model\", \"images\", name, str(it) +\".svg\")\n",
    "        plt.savefig(savepath, format=\"svg\")\n",
    "        it += 1\n",
    "    \n",
    "    return mask_for_eval\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c3882c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(mask_list, name):\n",
    "    scores = [name]\n",
    "    for mask, prediction in tqdm(mask_list):\n",
    "        score = functions.dice_loss(mask, prediction)\n",
    "        scores.append(score.item())\n",
    "    scores.append((sum(scores[1:])/15))\n",
    "    f = open(os.path.join(os.getcwd(),\"model\", \"images\", \"losses.csv\"), 'a')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(scores)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82844003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\antti\\Documents\\#1_Masters_studies\\LST_project\\GIT_REPO\\lst-project\\main.ipynb Cell 61'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/lst-project/main.ipynb#ch0000060?line=0'>1</a>\u001b[0m LC_mb_masks \u001b[39m=\u001b[39m make_predictions(Unet_LC_mb, CG_LC_mb_G_B2A, \u001b[39m\"\u001b[39;49m\u001b[39mLC_mb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/lst-project/main.ipynb#ch0000060?line=1'>2</a>\u001b[0m evaluate(LC_mb_masks, \u001b[39m\"\u001b[39m\u001b[39mLC_mb\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\antti\\Documents\\#1_Masters_studies\\LST_project\\GIT_REPO\\lst-project\\main.ipynb Cell 59'\u001b[0m in \u001b[0;36mmake_predictions\u001b[1;34m(Unet, CG, name)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/lst-project/main.ipynb#ch0000058?line=5'>6</a>\u001b[0m resize_256 \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mResize((\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/lst-project/main.ipynb#ch0000058?line=6'>7</a>\u001b[0m resize_orig \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mResize((\u001b[39m1544\u001b[39m, \u001b[39m2064\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/lst-project/main.ipynb#ch0000058?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m img, mask \u001b[39min\u001b[39;00m tqdm(test_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/lst-project/main.ipynb#ch0000058?line=9'>10</a>\u001b[0m     new_img \u001b[39m=\u001b[39m resize_256\u001b[39m.\u001b[39mforward(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/lst-project/main.ipynb#ch0000058?line=10'>11</a>\u001b[0m     new_img \u001b[39m=\u001b[39m CG(new_img)\n",
      "File \u001b[1;32mc:\\Users\\antti\\Documents\\#1_Masters_studies\\LST_project\\GIT_REPO\\LST\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/tqdm/std.py?line=1191'>1192</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/tqdm/std.py?line=1193'>1194</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/tqdm/std.py?line=1194'>1195</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/tqdm/std.py?line=1195'>1196</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/tqdm/std.py?line=1196'>1197</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/tqdm/std.py?line=1197'>1198</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\antti\\Documents\\#1_Masters_studies\\LST_project\\GIT_REPO\\LST\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\antti\\Documents\\#1_Masters_studies\\LST_project\\GIT_REPO\\LST\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\antti\\Documents\\#1_Masters_studies\\LST_project\\GIT_REPO\\LST\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\antti\\Documents\\#1_Masters_studies\\LST_project\\GIT_REPO\\LST\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\antti\\Documents\\#1_Masters_studies\\LST_project\\GIT_REPO\\lst-project\\src\\data_loader.py:152\u001b[0m, in \u001b[0;36mMaskedDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/lst-project/src/data_loader.py?line=148'>149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m DataLoaderException(\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/lst-project/src/data_loader.py?line=149'>150</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt read image \u001b[39m\u001b[39m{\u001b[39;00mimg_path\u001b[39m}\u001b[39;00m\u001b[39m. Make sure your data is located in \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_dir\u001b[39m}\u001b[39;00m\u001b[39m!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/lst-project/src/data_loader.py?line=150'>151</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/lst-project/src/data_loader.py?line=151'>152</a>\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39;49mload(mask_path))\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/lst-project/src/data_loader.py?line=152'>153</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/lst-project/src/data_loader.py?line=153'>154</a>\u001b[0m     \u001b[39mraise\u001b[39;00m DataLoaderException(\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/lst-project/src/data_loader.py?line=154'>155</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt read mask \u001b[39m\u001b[39m{\u001b[39;00mmask_path\u001b[39m}\u001b[39;00m\u001b[39m. Make sure your masks are located in \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_path\u001b[39m}\u001b[39;00m\u001b[39m!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\antti\\Documents\\#1_Masters_studies\\LST_project\\GIT_REPO\\LST\\lib\\site-packages\\numpy\\lib\\npyio.py:435\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/numpy/lib/npyio.py?line=431'>432</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/numpy/lib/npyio.py?line=432'>433</a>\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/numpy/lib/npyio.py?line=433'>434</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n\u001b[1;32m--> <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/numpy/lib/npyio.py?line=434'>435</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot load file containing pickled data \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/numpy/lib/npyio.py?line=435'>436</a>\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mwhen allow_pickle=False\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/numpy/lib/npyio.py?line=436'>437</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/antti/Documents/%231_Masters_studies/LST_project/GIT_REPO/LST/lib/site-packages/numpy/lib/npyio.py?line=437'>438</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m pickle\u001b[39m.\u001b[39mload(fid, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_kwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LC_mb_masks = make_predictions(Unet_LC_mb, CG_LC_mb_G_B2A, \"LC_mb\")\n",
    "evaluate(LC_mb_masks, \"LC_mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b1158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LC_mb_e_mask = make_predictions(Unet_LC_mb, CG_LC_mb_e_G_B2A, \"LC_mb_e\")\n",
    "evaluate(LC_mb_e_mask, \"LC_mb_e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2289751",
   "metadata": {},
   "outputs": [],
   "source": [
    "LC_mask =make_predictions(Unet_LC, CG_LC_G_B2A, \"LC\")\n",
    "evaluate(LC_mask, \"LC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "LC_e_mask = make_predictions(Unet_LC,CG_LC_e_G_B2A, \"LC_e\")\n",
    "evaluate(LC_e_mask, \"LC_e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c6c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "LC_mb_mix_mask = make_predictions(Unet_LC_mb_mix, CG_LC_mb_mix_G_B2A, \"LC_mb_mix\")\n",
    "evaluate(LC_mb_mix_mask, \"LC_mb_mix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3958c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03f9497c936f63cb81c8d62d1299b5500eac957543a5dc006a6c686138e88dad"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 ('LST')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
